{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5f3MY4iTdY_"
      },
      "source": [
        "# Fine-tune LLMs to do Sarcasm interpretations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk comet-ml emoji unbabel-comet datasets evaluate rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtYv_yON0u9y",
        "outputId": "b57de991-e145-40d4-ce3e-93b1adf29bdc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: comet-ml in /usr/local/lib/python3.10/dist-packages (3.47.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: unbabel-comet in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.32.3)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.17.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from comet-ml) (3.19.3)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.16.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (3.1.1)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (0.22.5)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (13.9.3)\n",
            "Requirement already satisfied: entmax<2.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.24.7)\n",
            "Requirement already satisfied: jsonargparse==3.13.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (3.13.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.2.2)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (4.25.5)\n",
            "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.4.0)\n",
            "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.4.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (0.10.3)\n",
            "Requirement already satisfied: transformers<5.0,>=4.17 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (4.44.2)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.12.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2024.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (2.18.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (75.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/SarcasmNLP')"
      ],
      "metadata": {
        "id": "kTEl5MCzmKfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79388c3-d122-49a6-ad31-a9cc7c8e8e5e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_choice = 'gpt2'\n",
        "model_choice = 'flan-t5-base'\n",
        "# model_choice = 't5-base'\n",
        "classifier_model_choice = 'bert-base-uncased'\n"
      ],
      "metadata": {
        "id": "35JsWx2OFaxk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = 'train'\n",
        "# mode = 'evaluate'"
      ],
      "metadata": {
        "id": "w4-U-25iLWIH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ = 'iSarcasm'\n",
        "# dataset_ = 'GPT-4o-mini'"
      ],
      "metadata": {
        "id": "lMBGEMhQO14U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9q8GFYTdZC"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classfication Model: bert-base-uncased"
      ],
      "metadata": {
        "id": "zyfZJA_5xnVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "if mode == 'train':\n",
        "  classifier_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  classifier_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "else:\n",
        "  classifier_tokenizer = BertTokenizer.from_pretrained(f'./results/{classifier_model_choice}/my_model')\n",
        "  classifier_model = BertForSequenceClassification.from_pretrained(f'./results/{classifier_model_choice}/my_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dthHsikNxwHX",
        "outputId": "c0a80f3d-3b31-41e0-e0fe-f096ca3e1dcb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation Models:"
      ],
      "metadata": {
        "id": "i8MYol6EyU-U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xASrnpTNTdZD"
      },
      "source": [
        "### GPT-2 small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "e43dmLCyTdZD"
      },
      "outputs": [],
      "source": [
        "if model_choice == 'gpt2':\n",
        "  from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "  if mode == 'train':\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "  else:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(f'./results/{model_choice}/my_model')\n",
        "    model = GPT2LMHeadModel.from_pretrained(f'./results/{model_choice}/my_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl3dt9S4TdZF"
      },
      "source": [
        "### Google FLAN-T5-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Vmie6yPATdZF"
      },
      "outputs": [],
      "source": [
        "if model_choice == 'flan-t5-base':\n",
        "  from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "  if mode == 'train':\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "  else:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(f'./results/{model_choice}/my_model')\n",
        "    model = T5ForConditionalGeneration.from_pretrained(f'./results/{model_choice}/my_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSA9hIn0TdZG"
      },
      "source": [
        "### T5-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ynp29onqTdZG"
      },
      "outputs": [],
      "source": [
        "if model_choice == 't5-base':\n",
        "  from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "  if mode == 'train':\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "  else:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(f'./results/{model_choice}/my_model')\n",
        "    model = T5ForConditionalGeneration.from_pretrained(f'./results/{model_choice}/my_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdR7px1KTdZG"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "RTOxH0rSTdZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def load_data(dataset):\n",
        "  # if dataset == 'iSarcasm':\n",
        "  #   return pd.read_csv('iSarcasm_pairs.tsv', sep='\\t')\n",
        "  # else:\n",
        "  #   return pd.read_csv('GPT_pairs.tsv', sep='\\t')\n",
        "  dataset = pd.read_csv('combined_df.tsv', sep='\\t')\n",
        "  evaluation_dataset = pd.read_csv('iSarcasm_pairs_test.tsv', sep='\\t')\n",
        "  dataset['Translation'] = dataset['Translation'].fillna('')\n",
        "  evaluation_dataset['Translation'] = evaluation_dataset['Translation'].fillna('')\n",
        "  return dataset, evaluation_dataset\n",
        "\n",
        "df, df_eval = load_data(dataset_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification model:"
      ],
      "metadata": {
        "id": "GZ5xYxrOzGvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intitialization"
      ],
      "metadata": {
        "id": "4ZjVGK0e0Puw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fZe6BR8AzmEV"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "rqvvSX6nTdZH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_texts(texts, targets=None, tokenizer=None):\n",
        "    if targets is None:  # For single input (sarcasm detection)\n",
        "        return classifier_tokenizer(\n",
        "            texts.tolist(),\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )"
      ],
      "metadata": {
        "id": "gPlqnc3wzXrR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Custom dataset class for sarcasm classification\n",
        "class SarcasmClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = encode_texts(texts.astype(str))\n",
        "        self.labels = torch.tensor(labels.values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.texts['input_ids'][idx],\n",
        "            'attention_mask': self.texts['attention_mask'][idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SarcasmClassificationDataset(train_df['Sarcastic'], train_df['IsSarcastic'])\n",
        "valid_dataset = SarcasmClassificationDataset(valid_df['Sarcastic'], valid_df['IsSarcastic'])\n",
        "test_dataset = SarcasmClassificationDataset(test_df['Sarcastic'], test_df['IsSarcastic'])"
      ],
      "metadata": {
        "id": "b4a8sbT_zY1Q"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "5NjXA7Y50V5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results/{classifier_model_choice}',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer for sarcasm classification\n",
        "trainer = Trainer(\n",
        "    model=classifier_model.to(device),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG_zXJH_z_44",
        "outputId": "2d98975c-27fe-40c2-8dc6-393ceb88f8a0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if mode == 'train':\n",
        "  # Train the sarcasm classifier\n",
        "  trainer.train()\n",
        "  # Save the model\n",
        "  classifier_model.save_pretrained(f'./results/{classifier_model_choice}/my_model')\n",
        "  classifier_tokenizer.save_pretrained(f'./results/{classifier_model_choice}/my_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "1B0lLUJS0DB5",
        "outputId": "3693f840-71bf-4d05-d262-e94b83aac730"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1704' max='2272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1704/2272 01:43 < 00:34, 16.37 it/s, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.366600</td>\n",
              "      <td>0.337337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.302500</td>\n",
              "      <td>0.407219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.217300</td>\n",
              "      <td>0.571988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "CmjD1Tf20F6y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "ac63ec76-f64f-43a2-ceb5-02834a2bcf02"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='144' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.33733686804771423, 'eval_runtime': 0.8989, 'eval_samples_per_second': 632.977, 'eval_steps_per_second': 80.095, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation Model:"
      ],
      "metadata": {
        "id": "KFjQC4xR0LkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "gXdq9WxA0ea4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "6dSJgjZ5vL_q"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_prefix = lambda x: \"Provide straightforward, literal translations for this sarcastic comment: \" + str(x)\n",
        "\n",
        "train_df['Input'] = train_df['Sarcastic'].apply(add_prefix)\n",
        "valid_df['Input'] = valid_df['Sarcastic'].apply(add_prefix)\n",
        "test_df['Input'] = test_df['Sarcastic'].apply(add_prefix)\n"
      ],
      "metadata": {
        "id": "1mE-QdW2QUol"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "2k7_NsCmTdZH"
      },
      "outputs": [],
      "source": [
        "train_df['Translation'] = train_df['Translation'].fillna(\"\")\n",
        "valid_df['Translation'] = valid_df['Translation'].fillna(\"\")\n",
        "test_df['Translation'] = test_df['Translation'].fillna(\"\")\n",
        "\n",
        "def tokenize_data(df):\n",
        "    inputs = tokenizer(df['Input'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    targets = tokenizer(df['Translation'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'labels': targets['input_ids'],\n",
        "    }\n",
        "\n",
        "# Tokenize train, validation, and test datasets\n",
        "train_encodings = tokenize_data(train_df)\n",
        "valid_encodings = tokenize_data(valid_df)\n",
        "test_encodings = tokenize_data(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "xJP1O4ilTdZH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SarcasmTranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SarcasmTranslationDataset(train_encodings)\n",
        "valid_dataset = SarcasmTranslationDataset(valid_encodings)\n",
        "test_dataset = SarcasmTranslationDataset(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SarcasmTranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, sources):\n",
        "        self.encodings = encodings\n",
        "        self.sources = sources\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['source'] = self.sources[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SarcasmTranslationDataset(train_encodings, list(train_df['Sarcastic']))\n",
        "valid_dataset = SarcasmTranslationDataset(valid_encodings, list(valid_df['Sarcastic']))\n",
        "test_dataset = SarcasmTranslationDataset(test_encodings, list(test_df['Sarcastic']))"
      ],
      "metadata": {
        "id": "2tIJ4EaglguX"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Metrics"
      ],
      "metadata": {
        "id": "iFrUtrv0SAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For colab, need to install additional packages (already in conda environment.yml)"
      ],
      "metadata": {
        "id": "SdvsHyUeUYAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "# Load the metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "comet = evaluate.load(\"comet\")  # Ensure COMET is installed and properly configured\n",
        "chrf = evaluate.load(\"chrf\")  # ChrF metric\n",
        "\n"
      ],
      "metadata": {
        "id": "YgbEHW5HR-xi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "e32b7ccb59b64cf0bb2e5e2ab0c914d8",
            "7287965d05a9458e874f59c7e988f0b0",
            "b0e066bc2a474defa7e29db8be2684c2",
            "b8e67dc42a70478faa94003547293ae6",
            "f2f1d961820c44679e1f85e29137b087",
            "aa45f7de760b47429671297c6b7c3517",
            "5bf68d899c9c472b994cecbfb7bd7f20",
            "6a5aaa648eec4affb8483ab0dee7ca56",
            "ae363b162c4e4218b9231279d13af3df",
            "b91aacd48b474f8ba1ad20160ce9dabb",
            "0bed11f8d25d4038a32738c6b3f2b970"
          ]
        },
        "outputId": "c988dfd4-dd8d-409a-b81c-d6d587493aae"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e32b7ccb59b64cf0bb2e5e2ab0c914d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_metrics(decoded_preds, decoded_labels, sources):\n",
        "    # # Get predictions and labels\n",
        "    # predictions = pred.predictions[0]\n",
        "    # labels = pred.label_ids\n",
        "    # # if isinstance(predictions, list) and isinstance(predictions[0], list):\n",
        "    # #     predictions = [pred[0] for pred in predictions]\n",
        "\n",
        "    # # Decode predictions and labels\n",
        "    # decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # ChrF\n",
        "    chrf_result = chrf.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # COMET\n",
        "    comet_result = comet.compute(predictions=decoded_preds, references=decoded_labels, sources=sources)\n",
        "\n",
        "\n",
        "\n",
        "    # Combine the results, including all ROUGE scores\n",
        "    metrics = {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"chrf\": chrf_result[\"score\"],\n",
        "        \"comet\": comet_result.get(\"mean_score\", None),\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"rougeLsum\": rouge_result.get(\"rougeLsum\", None),\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "q7g_1_YRUBEb"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO1dwIB0TdZI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "0rkyWqoAOXs3"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "O6AnvT_7TdZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1da4d7e-a2ff-42b9-d487-79bc083d5ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'google/flan-t5-base'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "model.name_or_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "2BrFL5kYTdZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93de2d35-901e-4115-a8fa-241ec21650c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'./results/{model_choice}',\n",
        "    evaluation_strategy=\"epoch\",     # evaluation strategy to adopt during training\n",
        "    learning_rate=2e-5,              # learning rate\n",
        "    save_steps=10000,\n",
        "    save_total_limit=1,              # keep only the most recent checkpoint\n",
        "    per_device_train_batch_size=8,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    num_train_epochs=10,             # total number of training epochs\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",                # Disable wandb logging\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    # compute_metrics=compute_metrics, # TODO change it as batch evaluation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "Gpu4DARFTdZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "aa3206b5-8d62-48fa-ac6b-1144237db0c5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3130/3130 08:50, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.150400</td>\n",
              "      <td>0.151438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.134300</td>\n",
              "      <td>0.152384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.144700</td>\n",
              "      <td>0.152622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.148700</td>\n",
              "      <td>0.152599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.159800</td>\n",
              "      <td>0.152677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.134800</td>\n",
              "      <td>0.152775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.114700</td>\n",
              "      <td>0.153192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.124300</td>\n",
              "      <td>0.153323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.132700</td>\n",
              "      <td>0.153351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.123900</td>\n",
              "      <td>0.153373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if mode == 'train':\n",
        "  trainer.train()\n",
        "  # Save the model\n",
        "  model.save_pretrained(f'./results/{model_choice}/my_model')\n",
        "  tokenizer.save_pretrained(f'./results/{model_choice}/my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "CQn4mY5txO3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "Kj-0ol7SJi4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "2ee78947-91e4-47b0-e092-72242c90d20b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.15337307751178741, 'eval_runtime': 1.8435, 'eval_samples_per_second': 169.788, 'eval_steps_per_second': 21.698, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels, _ = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "id": "vx2poj6pSPj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "427c1902-db63-48db-a81b-459900f69a75"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor(predictions[0])\n",
        "\n",
        "# Select the token with the highest probability\n",
        "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Decode the predicted tokens\n",
        "decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "9DwqgDmZUZHb"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [test_dataset[i]['source'] for i in range(len(test_dataset))]"
      ],
      "metadata": {
        "id": "TmC2SE-JlJpw"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = compute_metrics(decoded_preds, decoded_labels, sources)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "xE2mRGmtUbml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed410c1-68ff-4c42-a0f7-57590e4198df"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.29065480751157347, 'chrf': 54.2714459428979, 'comet': 0.6346741736696931, 'rouge1': 0.5952492289752125, 'rouge2': 0.3542191810498126, 'rougeL': 0.5883517830192877, 'rougeLsum': 0.588485333106421}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = compute_metrics(sources, decoded_labels, sources)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "0pLaFaX1ouEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70ffded-9f55-4e21-931a-9b41f78ec0c8"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.05456304383967352, 'chrf': 30.46879140238477, 'comet': 0.6295675352549019, 'rouge1': 0.23279939827351492, 'rouge2': 0.08526930014033529, 'rougeL': 0.20343531537645942, 'rougeLsum': 0.20301373500470155}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = compute_metrics(decoded_preds, sources, sources)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "SI2ksXGPo58y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4388c4d-13a3-4752-afe0-ec4cbd0613d4"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.056556260320132654, 'chrf': 25.74152360437061, 'comet': 0.4818565776934639, 'rouge1': 0.25204569006912597, 'rouge2': 0.09119669403102376, 'rougeL': 0.22355581225684276, 'rougeLsum': 0.2237242007879779}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def inference(input_text):\n",
        "#   if not input_text.startswith(\"Provide straightforward, literal translations for this sarcastic comment: \"):\n",
        "#     input_text = \"Provide straightforward, literal translations for this sarcastic comment: \" + input_text\n",
        "#   # input_text = \"Provide straightforward, literal translations for this sarcastic comment: I just absolutely LOVE how I've got to work outside for the next 3 days in the heatwave.\"\n",
        "\n",
        "#   input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "#   output_ids = model.generate(input_ids)\n",
        "#   decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "#   return decoded_output\n",
        "\n",
        "def classify_sarcasm(text):\n",
        "    inputs = classifier_tokenizer(text, return_tensors=\"pt\").to(device)  # Move inputs to the same device as the model\n",
        "    # Move the model to the same device as the inputs\n",
        "    classifier_model.to(device)\n",
        "    outputs = classifier_model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return prediction == 1\n",
        "\n",
        "def generate_interpretation(input_text):\n",
        "    if not input_text.startswith(\"Provide straightforward, literal translations for this sarcastic comment: \"):\n",
        "      input_text = \"Provide straightforward, literal translations for this sarcastic comment: \" + input_text\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "    output_ids = model.generate(input_ids)\n",
        "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output\n"
      ],
      "metadata": {
        "id": "n2WBwsaTKIl7"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on a few examples\n",
        "test_examples = df_eval.sample(n=20, random_state=42)\n",
        "for index, row in test_examples.iterrows():\n",
        "    is_sarcastic = classify_sarcasm(row['Sarcastic'])\n",
        "    print(f\"Original: {row['Sarcastic']} {is_sarcastic}\")\n",
        "    if is_sarcastic:\n",
        "        interpretation = generate_interpretation(row['Sarcastic'])\n",
        "        print(f\"Interpretation: {interpretation}\\n\")\n",
        "        print(f\"Ground truth: {row['Translation']}\\n\")\n",
        "    else:\n",
        "        print(\"Not Sarcastic\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttnmxlDaafQn",
        "outputId": "c2537ab7-4833-410a-ae2f-297f2746f663"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: i’m dying False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Mental shoot out ends. Silent pause. Sirens start 🚨 Always so perfectly timed 🚔🚓👍🏽 #LineOfDuty #LineofDuty6 #LOD6 #LOD False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: using loops, building an idea on that loop, and then deleting the loop has been a fun little exercise recently True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpretation: I would say \"Using loops, building an idea on that loop, and then deleting\n",
            "\n",
            "Ground truth: \n",
            "\n",
            "Original: \"Alexa add small bananas to the shopping list.\" I've added smooth armours to your shopping list.\" Gee thanks Alexa! #alexa False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Just rediscovered that my nose is always in my field of vision so I will unfortunately be dropping out as I cannot continue my assigned readings in peace False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: I am starting off my day by watching a Dragons Den special on Deborah Meaden and eating a Pot Noodle. My life is perfect. False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: not to brag, but my thesis topic was so good that someone else published on it three months ago 🙃 False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: @miiniigun I feel that 100%, my anxiety stops me from talking to so many people. I've let so many friendships die because I can't push myself to talk to them. False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Day 38 of quarantine is an interesting time to get a new drum kit, neighbor False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Being able to open all of the windows today because it’s warm makes me so 🥺🥺🥺 False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Where is the Chancellor? Please check the back of your sofas. True\n",
            "Interpretation: I would say \"Where is the Chancellor?\"\n",
            "\n",
            "Ground truth: (!) I wonder where the Chancellor is after such an important decision?\n",
            "\n",
            "Original: I love leaving the doctors office in tears. I need this kid to just decide to leave my body so we can be done and home together. True\n",
            "Interpretation: I hate leaving the doctors office in tears. I need this kid to just decide to leave my\n",
            "\n",
            "Ground truth: I hate leaving the doctors office crying. \n",
            "\n",
            "Original: Every time I see an establishment with paper straws I turn a little bit Republican False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: So weird how Venmo hasn’t venmoed me yet really thought I was the one False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Bring on diallo ffs. Nothing is working rn. False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Yeah I’m following a lot of shows right now, the price is right is pretty good but season 1 is better than season 2 False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Some pretty great things are happening at the call center. Listen in to 104.5 for the Care-A-Thon! Each dollar is being matched, my friends! True\n",
            "Interpretation: I'm annoyed that the Care-A-Thon isn't being matched\n",
            "\n",
            "Ground truth: \n",
            "\n",
            "Original: every date I go on I get a little scared will be phone swap False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: imagine if he sends a good morning message when he wakes up False\n",
            "Not Sarcastic\n",
            "\n",
            "Original: Headaches that last all day nonstop🥰😍 False\n",
            "Not Sarcastic\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = \"Look at you, finishing all your snacks before dinner. What a healthy choice!\"\n",
        "truth = \"Eating snacks before dinner is not a good decision for your health.\"\n",
        "print(f\" \\nsrc: {src} \\nisSarcastic: {classify_sarcasm(src)} \\ntranslation: {generate_interpretation(src)} \\nground_truth: {truth}\")\n"
      ],
      "metadata": {
        "id": "AGMa1oOxeDWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1e23b8-deda-4702-b779-7a73f910bdda"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "src: Look at you, finishing all your snacks before dinner. What a healthy choice! \n",
            "isSarcastic: True \n",
            "translation: Finishing snacks before dinner is not healthy. \n",
            "ground_truth: Eating snacks before dinner is not a good decision for your health.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate on GPT-4o-mini pairs"
      ],
      "metadata": {
        "id": "Ei0XSVvIQihW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2, eval_data = load_data(\"GPT-4o-mini\")\n",
        "df2['Input'] = df2['Sarcastic'].apply(add_prefix)"
      ],
      "metadata": {
        "id": "yTDYaLy9QnGP"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_pairs = SarcasmTranslationDataset(tokenize_data(df2))"
      ],
      "metadata": {
        "id": "rExMh8EIQh6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_temp = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=gpt_pairs,\n",
        "    tokenizer=tokenizer,\n",
        "    # compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "iT4xfu7XMb5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "iC4C2iIIRCHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "c744a2fa-8c72-436d-aa5c-c393960b2edd"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.15337307751178741, 'eval_runtime': 1.5312, 'eval_samples_per_second': 204.418, 'eval_steps_per_second': 26.124, 'epoch': 10.0}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e32b7ccb59b64cf0bb2e5e2ab0c914d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7287965d05a9458e874f59c7e988f0b0",
              "IPY_MODEL_b0e066bc2a474defa7e29db8be2684c2",
              "IPY_MODEL_b8e67dc42a70478faa94003547293ae6"
            ],
            "layout": "IPY_MODEL_f2f1d961820c44679e1f85e29137b087"
          }
        },
        "7287965d05a9458e874f59c7e988f0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa45f7de760b47429671297c6b7c3517",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf68d899c9c472b994cecbfb7bd7f20",
            "value": "Fetching 5 files: 100%"
          }
        },
        "b0e066bc2a474defa7e29db8be2684c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5aaa648eec4affb8483ab0dee7ca56",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae363b162c4e4218b9231279d13af3df",
            "value": 5
          }
        },
        "b8e67dc42a70478faa94003547293ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91aacd48b474f8ba1ad20160ce9dabb",
            "placeholder": "​",
            "style": "IPY_MODEL_0bed11f8d25d4038a32738c6b3f2b970",
            "value": " 5/5 [00:00&lt;00:00, 388.66it/s]"
          }
        },
        "f2f1d961820c44679e1f85e29137b087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa45f7de760b47429671297c6b7c3517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf68d899c9c472b994cecbfb7bd7f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a5aaa648eec4affb8483ab0dee7ca56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae363b162c4e4218b9231279d13af3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b91aacd48b474f8ba1ad20160ce9dabb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bed11f8d25d4038a32738c6b3f2b970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}